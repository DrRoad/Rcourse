```{r, include=FALSE}
library(lubridate)
date <- date(now())
```

TensorFlow
========================================================
author: Wim van der Ham
date: `r date`
autosize: true

What is TensorFlow?
========================================================

- General purpose numerical computing library (but used a lot for neural networks)
- Open source
- Not all data has to be in RAM
- Hardware independent (CPU, GPU, TPU)
- Can distribute computations over different hardware
- Fast low level computations **c++**
- R can be really useful as an [interface](https://tensorflow.rstudio.com/)

Tensors
========================================================

> Data stored in a multidimensional array

```{r, eval=FALSE}
# 0D
42
# 1D
c(42, 42, 42)
# 2D
matrix(42, nrow = 2, ncol = 2)
iris
# 3D
# example: how features change over time per sample
array(42, dim = c(2, 3, 3))
# 4D example: image -> weight, height, color, sample
# 5D example: video -> weight, height, color, sample, time
```

Deep Learning
========================================================

- Go from input to output using several layers
- A layer is a data transformation
- Feature engineering is learned in stead of hard coded

Because there are multiple layers the learning is called **deep**

Deep Learning Training
========================================================

[explanation using a video](https://www.youtube.com/watch?v=R9OHn5ZF4Uo)

1. Layers are initialized with random weights
1. Predictions are made and compared with the true output
1. Optimizer is used to update the weights using the results from the loss function

Techniques are know for a long time already, but the trick was to use really large models with large amount of data (2012)

Deep Learning Downsides
========================================================

- Black box
- Needs a large amount of data
- Computational expensive

API's for TensorFlow
========================================================

1. **Keras API** High level packages for Deep Learning
1. **Estimator API** More classical models
1. **Core API** Low level access

Layers
========================================================

- **Dense** traditional neural networks
- **Convolution** used for image recognition, searching for spacial features
- **Recurrent** maintain a state, used in sequence oriented applications like text recognition
- **Embedding** using classifications for words, can be pre-trained like Word2vec or GloVe

Finding the right layers for the application is what costs most of the time

Why use the Cloud for Computing?
========================================================

1. Your data is in the cloud
1. You need more computation resources for a short or longer time
1. You need fast computation

Which services can you use?
========================================================

Full control over the machine using [`tfruns`](https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html)

- [Paperspace](https://www.paperspace.com/)
- [Amazon AWS](https://aws.amazon.com)

Only trains your model and gives back the results, no control over the machine using [`cloudml`](https://tensorflow.rstudio.com/tools/cloudml/articles/getting_started.html)

- [Google ML](https://cloud.google.com/products/machine-learning/)

tfruns::training_run()
========================================================

Use `training_run()` to keep track of your experimentation

After that you can use

- `ls_runs()` to view all the runs you did
- `view_run()` to view one specific run
- `compare_run()` to compare runs with each other

tfruns::flags()
========================================================

1. Use `flags()` to create a list of parameters that are used in the model.
1. For different runs a different flag list can be given as argument in  the `training_run()` function
1. After the different runs can be compared.

cloudml
========================================================

1. Use `cloudml::cloudml_train()`. Similar to `tfruns::training_run()` only have to supply a `master_type` to specify the machine that will be used.
1. Use `cloudml::cloudml_train()` with a config.yml file for tuning
1. `cloudml::job_trails()` ~ `tsruns::ls_runs()`
1. `cloudml::job_collect()` ~ `tsruns::view_run()`
