---
title: "Introducing sparklyr"
output: html_document
---

## Setup

- Install updated versions of `tidyverse` and `sparklyr`

```{r, eval = FALSE}
  install.packages("sparklyr")
  install.packages("tidyverse")
```

- Load the libraries

```{r}
  library(sparklyr)
  library(tidyverse)
```

- Install Spark version 2.1.0 locally in your computer

```{r, eval = FALSE}
  spark_install(version = "2.1.0")
```

## Create a Spark session

We will use a custom configuration for the `sparklyr` connection, we are requesting:

- 16 gigabytes of memory for the `driver`
- Make 80% of the memory accessible to use during the analysis

```{r}
  conf <- spark_config()
  conf$`sparklyr.shell.driver-memory` <- "7G" 
  conf$spark.memory.fraction <- 0.8
```

- Make sure to pass the `conf` variable as the value for the `config` argument
- Navigate to http://127.0.0.1:4040/executors/
- In the **Executors** section there is 5.1 GB of Storage Memory assigned (7 * 80%)
- There are also 4 cores assigned

```{r}
  sc <- spark_connect(master = "local", config = conf, version = "2.1.0")
```

## Copy data into Spark

`write_csv()` function can be used to write data from R to a csv file. After that the csv file can be imported.

```{r}
library(nycflights13)
flights <- as_data_frame(flights)
write_csv(flights, "./flights.csv", na = "NA")
```

## Load data

Creates a new table using the previously written csv file in the Spark environment called `flights`

```{r}
sp_flights <- spark_read_csv(sc,
                             name = "flights",
                             path = "./flights.csv",
                             memory = FALSE)
sp_flights_memory <- spark_read_csv(sc,
                             name = "flights_mem",
                             path = "./flights.csv",
                             memory = TRUE)
```

- See http://127.0.0.1:4040/storage/, to confirm that only when `memory = TRUE` the file is in the Spark storage.

## Spark SQL

- Use the `DBI` package for SQL operations in `sparklyr`
- `dbGetQuery()` pulls the data into R automatically

```{r}
  library(DBI)
 
  top10 <- dbGetQuery(sc, "Select * from flights limit 10")
 
  top10
```

### Use SQL in a code chunk

- RMarkdown allows non-R chunks like SQL: http://rmarkdown.rstudio.com/authoring_knitr_engines.html#sql

```{sql, connection = sc}
  SELECT  * FROM flights WHERE origin = "EWR" LIMIT 10
```

- With output var

```{sql, connection = sc, output.var='top_10_ewr'}
  SELECT  * FROM flights WHERE origin = "EWR" LIMIT 10
```

### dplyr

- Use `dplyr` verbs to interact with the data
- Use `show_query()` to see the sql of the `dplyr` command

```{r}
flights_table <- sp_flights %>%
  mutate(dep_delay = as.numeric(dep_delay),
         arr_delay = as.numeric(arr_delay),
         sched_dep_time = as.numeric(sched_dep_time)) %>%
  select(origin, dest, sched_dep_time, sched_arr_time,
         arr_delay, dep_delay, month, distance)
# %>%  show_query()
flights_table %>% head
```

### Store Data

Use a spark write function to store the data in HDFS or s3.

```{r}
  spark_write_csv(flights_table, "csv_files")
  spark_write_parquet(flights_table, "parquet_files")
  spark_write_json(flights_table, "json_files")
```

### Compute

-`compute()`  caches a Spark DataFrame into memory
- It performs these two operations: `sdf_register()` + `tbl_cache()`
- After the code below completes, see http://127.0.0.1:4040/storage/, the is a new table called `flights_subset`

```{r}
  subset_table <- flights_table %>%
    compute("flights_subset")
```

- Now we can perform more complex aggregations

```{r}
subset_table %>%
  group_by(dest) %>%
  summarise(n = n(),
            mean_sdt = mean(sched_dep_time),
            mean_sat = mean(sched_arr_time))
```

## Improved sampling

```{r}
# Improved in sparklyr 0.6
  sp_flights %>%
    sample_frac(0.0001) %>%
    show_query()
```

```{r}
  sp_flights %>%
    sample_frac(0.0001) %>%
    group_by(Year) %>%
    tally
```

## Spark DataFrame (sdf) Functions

### sdf_pivot()

New in `sparklyr` 0.6! - Construct a pivot table over a Spark Dataframe, using a syntax similar to that from `reshape2::dcast()` and `tidyr::spread`

```{r}
subset_table %>%
  group_by(origin, dest) %>%
  tally() %>%
  head()
```

```{r}
subset_table %>%
  filter(origin == "EWR" | origin == "JFK") %>%
  sdf_pivot(origin~dest)
```

## Feature Transformers (ft)

https://spark.apache.org/docs/latest/ml-features.html

### ft_binarizer()

- Apply threshold to a column, such that values less than or equal to the threshold are assigned the value 0.0, and values greater than the threshold are assigned the value 1.0.
- [*The Federal Aviation Administration (FAA) considers a flight to be delayed when it is 15 minutes later than its scheduled time.*](https://en.wikipedia.org/wiki/Flight_cancellation_and_delay)

```{r}
subset_table %>%
  ft_binarizer(input.col =  "dep_delay",
               output.col = "delayed",
               threshold = 15) %>%
  head(200)
```

### ft_bucketizer()

- Similar to R's `cut()` function, this transforms a numeric column into a discretized column, with breaks specified through the splits parameter.

```{r}
subset_table %>%
  ft_bucketizer(input.col =  "sched_dep_time",
                output.col = "DepHour",
                splits = c(0, 400, 800, 1200, 1600, 2000, 2400)) %>%
  head(100)
```

## MLib

- `sparklyr` enables us to use `dplyr` verbs, `sdf` functions and `ft` functions to prepare data within a single piped code segment

```{r}
sample_data <- subset_table %>%
  filter(!is.na(dep_delay)) %>%
  ft_binarizer(input.col = "dep_delay",
               output.col = "delayed",
               threshold = 30) %>%
  ft_bucketizer(input.col =  "sched_dep_time",
                output.col = "DepHour",
                splits = c(0, 400, 800, 1200, 1600, 2000, 2400)) %>%
  mutate(DepHour = paste0("h", as.integer(DepHour))) %>%
  sdf_partition(training = 0.001, testing = 0.009, other = 0.99)
```

```{r}
  training <- compute(sample_data$training, "training")
```

- A formula can be used for modeling, as in: `x ~ y + z`

```{r}
delayed_model <- ml_logistic_regression(training, delayed ~ dep_delay + DepHour)
summary(delayed_model)
# or use ml_summary(delayed_model) for more details
```

- We will use the `testing` sample to run predictions
- It returns the same Spark DataFrame but with new columns

```{r}
delayed_testing <- sdf_predict(sample_data$testing, delayed_model)
delayed_testing %>% head
```

To explore this dataframe in R it must be imported first using `collect()`

```{r}
delayed_testing_r <- delayed_testing %>%
  collect()
delayed_testing_r$prediction
```

- Let's see how the model performed

```{r}
delayed_testing %>%
  group_by(delayed, prediction) %>%
  summarise(n = n())
```

## dbplot

https://db.rstudio.com/dbplot/

```{r}
library(dbplot)

flights_table %>% 
  dbplot_histogram(sched_dep_time)

flights_table %>%
  filter(!is.na(arr_delay)) %>%
  dbplot_raster(arr_delay, dep_delay) 

flights_table %>%
  filter(!is.na(arr_delay)) %>%
  dbplot_raster(arr_delay, dep_delay, mean(distance, na.rm = TRUE)) 

flights_table %>%
  dbplot_bar(origin)

flights_table %>%
  dbplot_line(month)

flights_table %>%
  dbplot_boxplot(origin, dep_delay)
```

## Distributed R

`spark_apply()` applies an R function to a Spark object.  The R function runs over each RDD in Spark. Please read this article: https://spark.rstudio.com/articles/guides-distributed-r.html

- The `training` Spark DataFrame has 4 partitions, `nrow()` will run in each partition

```{r}
  training %>%
    spark_apply(nrow)
```

### Group by

- The `group_by` argument can be used to run the R function over a specific column or columns instead of the RDD partitions.

```{r}
training %>%
  spark_apply(nrow, group_by =  "month", columns = "count")
```

## Distributing Packages

- With spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data from a `glm()` model output.

```{r}
spark_apply(
  training,
  function(e) broom::tidy(glm(delayed ~ arr_delay, data = e, family = "binomial")),
  names = c("term", "estimate", "std.error", "statistic", "p.value"),
  group_by = "origin")
```

